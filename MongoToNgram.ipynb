{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MongoDB insert all jurybook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import _uniout\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "#make a connection to mongodb \n",
    "client = pymongo.MongoClient('localhost',27017)\n",
    "#assign db_name to db\n",
    "db = client.test\n",
    "#make the collection in the db of test\n",
    "samples = db['samples']\n",
    "#print the case number in db.dday\n",
    "content = u''\n",
    "for dic in db.samples.find():\n",
    "    #print _uniout.unescape(str(i),'utf-8') \n",
    "    content += dic['content'] #append all the jurybook to the content\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cut the word by all apending jurybook\n",
    "\n",
    "# Please put the \"stopwords.txt\" with the same directory of ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#-*- utf-8 -*-\n",
    "import codecs\n",
    "import operator\n",
    "import os\n",
    "\n",
    "cutlist = \" <>/:：;；,、＂’，.。！？｢\\\"\\'\\\\\\n\\r《》“”!@#$%^&*()\".decode(\"utf-8\")  ##列出標點符號，並轉換成utf-8的格式\n",
    "def cutSentence(content, stopwords): ##放入原始文章路徑, 增加斷詞的list\n",
    "    sentence = \"\"\n",
    "    textList = []\n",
    "       \n",
    "    for line in content:\n",
    "        line = line.strip() ##清除空白\n",
    "        #line = line.replace(' ', '')\n",
    "        for word in line:\n",
    "            if word not in cutlist: #如果文字不是標點符號，就把字加到句子中\n",
    "                sentence += word\n",
    "                #print sentence\n",
    "            else:\n",
    "                #stopwords =[u'裁判']\n",
    "                for stopword in stopwords:  #清除關鍵字\n",
    "                    #print stopword\n",
    "                    sentence = sentence.replace(stopword,'')\n",
    "                    #print type(stopword)\n",
    "                #print sentence\n",
    "                textList.append(sentence) #把文章句子依標點符號與空白隔開\n",
    "                sentence = \"\"\n",
    "            #for ele in textList: #顯示文章所斷句子\n",
    "                #print ele\n",
    "    return textList\n",
    "    \n",
    "def ngram(textLists,n,minFreq): \n",
    "    #首參數放處理好的文章(LIST檔，utf-8編碼)次參數放字詞的長度單位，第三個參數放多少頻次以上\n",
    "    words=[]     #存放擷取出來的字詞\n",
    "    words_freq={}#存放字詞:計算個數 \n",
    "    result= []\n",
    "    for textList in textLists:\n",
    "        for w in range(len(textList)-(n-1)): #要讀取的長度隨字詞長度改變\n",
    "            words.append(textList[w:w+n])    #抓取長度w-(n-1)的字串\n",
    "\n",
    "    for word in words:\n",
    "        if word not in words_freq:               #如果這個字詞還沒有被放在字典檔中\n",
    "            words_freq[word] = words.count(word) #就開一個新的字詞，裡面放入字詞計算的頻次\n",
    " \n",
    "    words_freq = sorted(words_freq.iteritems(),key=operator.itemgetter(1),reverse=True) #change words_freq from dict to list \n",
    "    \n",
    "    for word in words_freq:\n",
    "        if word[1] >= minFreq:\n",
    "            result.append(word)\n",
    "    return result ##回傳一個陣列[(詞,頻次),(詞,頻次)]\n",
    "\n",
    "def longTermPriority(path, maxTermLength, minFreq):   #目前設定只傳回長詞longTerms\n",
    "    stopwords=[]\n",
    "    with codecs.open('./stopwords.txt','r' ,'utf-8') as f :\n",
    "        readstopwords = f.readlines() #從txt檔讀取每行的停止詞\n",
    "    for ele in readstopwords:\n",
    "        stopwords.append(ele.strip())#從每個停止詞中，去除該死的換行符號\n",
    "    longTermsFreq=[]      #長詞+次數分配\n",
    "    for i in range(maxTermLength,1,-1): ##字詞數由大至小\n",
    "        text_list = cutSentence(path,stopwords)  \n",
    "        words_freq = ngram(text_list,i, minFreq) \n",
    "        for word_freq in words_freq:\n",
    "            longTermsFreq.append(word_freq) #將長詞和次數加入另外一個list  分成兩個檔儲存的用意是減少迴圈次數\n",
    "    return longTermsFreq\n",
    "\n",
    "longTermFreq = longTermPriority(content,4,2) ##最長詞5個字、出現頻次1次以上\n",
    "#print longTermFreq\n",
    "#for i in longTermFreq:\n",
    " #   print i[0],i[1]   #印出斷字結果\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 切字結果轉存至記事本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import _uniout\n",
    "import codecs,os\n",
    "try:\n",
    "    os.remove('ngramwords.txt')\n",
    "except:\n",
    "    pass\n",
    "for i in longTermFreq:\n",
    "    #tt = str(i[1]).decode('utf8')\n",
    "    #print type(tt)\n",
    "    f = codecs.open('ngramwords.txt','a+','utf-8')\n",
    "    llist = i[0]+','+str(i[1]).decode('utf8')+'\\n'   #印出斷字結果\n",
    "    f.write(llist)\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
