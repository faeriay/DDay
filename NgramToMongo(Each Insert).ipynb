{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ngram Dict insert to Mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#-*- utf-8 -*-\n",
    "import codecs\n",
    "import operator\n",
    "import os\n",
    "import re\n",
    "import _uniout\n",
    "\n",
    "def no_court(content):\n",
    "    try:\n",
    "        court_no = {u\"臺北地方\":100,u\"士林地方\":111,u\"新北地方\":220,u\"宜蘭地方\":260,u\"基隆地方\":200,u\"桃園地方\":320,u\"新竹地方\":300,u\"苗栗地方\":350,u\"臺中地方\":400,u\"彰化地方\":500,u\"南投地方\":540,u\"臺南地方\":700,u\"高雄地方\":800,u\"花蓮地方\":970,u\"雲林地方\":630,u\"嘉義地方\":602,u\"臺東地方\":950,u\"屏東地方\":900,u\"澎湖地方\":880,u\"金門地方\":890,u\"連江地方\":209,u\"高雄少年及家事\":850}\n",
    "        m = re.search(u'[  \\d]*(灣|福建|臺灣)(\\S{4,7})法院(民事|家事|暫時|支付)(\\S{2})',content)\n",
    "        return str(court_no[m.group(2)])\n",
    "    except:\n",
    "        print \"Rex Err:\"+m.group(2)\n",
    "def no_jurytype(content):\n",
    "    try:\n",
    "        jurytype_no = {u\"婚\":11,u\"婚再\":12,u\"婚更\":13,u\"家聲\":21,u\"家訴\":22,u\"家抗\":23,u\"家陸許\":24,u\"司家聲\":30,u\"基家簡\":25,u\"重家訴\":26,u\"婚更一\":27,u\"婚更(一)\":28,u\"司家調\":29,u\"家移調\":31,u\"家簡\":32,u\"家調裁\":33,u\"家查\":34,u\"家聲抗\":35,u\"家調裁\":36,u\"家調補\":37,u\"司家補\":38,u\"家親聲\":39,u\"司家簡調\":40,u\"家簡上\":41,u\"家小\":42,u\"家救\":43,u\"家小上\":44,u\"家婚聲\":45,u\"家再\":46,u\"家他\":47,u\"訴\":48,u\"重訴\":49,u\"家續\":50,u\"審訴\":51,u\"聲\":52,u\"補\":90}\n",
    "        m = re.search(u'(【裁判字號】)(.*?),(.*?),(\\d{1,4})[^【】 ]',content)\n",
    "        return m.group(2)+\"_\"+str(jurytype_no[m.group(3)])+\"_\"+m.group(4)\n",
    "    except:\n",
    "        print \"Rex Err:\"+m.group(2)\n",
    "def judgedate(content):\n",
    "    m = re.search(u'(【裁判日期】)(\\d{6,7})',content)\n",
    "    return m.group(2)\n",
    "\n",
    "def getPK(content):\n",
    "    return no_court(content)+\"_\"+no_jurytype(content)+\"_\"+judgedate(content)\n",
    "\n",
    "cutlist = \" <>/:：;；,、＂’，.。！？｢\\\"\\'\\\\\\n\\r《》“”!@#$%^&*()\".decode(\"utf-8\")  ##列出標點符號，並轉換成utf-8的格式\n",
    "def cutSentence(content, stopwords): ##放入原始文章路徑, 增加斷詞的list\n",
    "    sentence = \"\"\n",
    "    textList = []\n",
    "       \n",
    "    for line in content:\n",
    "        line = line.strip() ##清除空白\n",
    "        #line = line.replace(' ', '')\n",
    "        for word in line:\n",
    "            if word not in cutlist: #如果文字不是標點符號，就把字加到句子中\n",
    "                sentence += word\n",
    "                #print sentence\n",
    "            else:\n",
    "                #stopwords =[u'裁判']\n",
    "                for stopword in stopwords:  #清除關鍵字\n",
    "                    #print stopword\n",
    "                    sentence = sentence.replace(stopword,'')\n",
    "                    #print type(stopword)\n",
    "                #print sentence\n",
    "                textList.append(sentence) #把文章句子依標點符號與空白隔開\n",
    "                sentence = \"\"\n",
    "            #for ele in textList: #顯示文章所斷句子\n",
    "                #print ele\n",
    "    return textList\n",
    "    \n",
    "def ngram(textLists,n): \n",
    "    #首參數放處理好的文章(LIST檔，utf-8編碼)次參數放字詞的長度單位，第三個參數放多少頻次以上\n",
    "    words=[]     #存放擷取出來的字詞\n",
    "    words_freq={}#存放字詞:計算個數 \n",
    "    result= []\n",
    "    for textList in textLists:\n",
    "        for w in range(len(textList)-(n-1)): #要讀取的長度隨字詞長度改變\n",
    "            words.append(textList[w:w+n])    #抓取長度w-(n-1)的字串\n",
    "\n",
    "    for word in words:\n",
    "        if word not in words_freq:               #如果這個字詞還沒有被放在字典檔中\n",
    "            words_freq[word] = words.count(word) #就開一個新的字詞，裡面放入字詞計算的頻次\n",
    " \n",
    "    #words_freq = sorted(words_freq.iteritems(),key=operator.itemgetter(1),reverse=True) #change words_freq from dict to list \n",
    "    \n",
    "    #for word in words_freq:\n",
    "        #if word[1] >= minFreq:\n",
    "            #result.append(word)\n",
    "    return words_freq ##回傳一個陣列[(詞,頻次),(詞,頻次)]\n",
    "\n",
    "def longTermPriority(content, maxTermLength,minTermLength, minFreq):   #目前設定只傳回長詞longTerms\n",
    "    stopwords=[]\n",
    "    words_freq = {}\n",
    "    brokenwords = {}\n",
    "    mincut = minTermLength-1\n",
    "    with codecs.open('./ngram/stopwords.txt','r' ,'utf-8') as f :\n",
    "        readstopwords = f.readlines() #從txt檔讀取每行的停止詞\n",
    "    for ele in readstopwords:\n",
    "        stopwords.append(ele.strip())#從每個停止詞中，去除該死的換行符號\n",
    "    longTermsFreq=[]      #長詞+次數分配\n",
    "    for i in range(maxTermLength,3,-1): ##字詞數由大至小\n",
    "        text_list = cutSentence(content,stopwords)  \n",
    "        words_freq.update(ngram(text_list,i))\n",
    "        #for word_freq in words_freq:\n",
    "            #longTermsFreq.append(word_freq) #將長詞和次數加入另外一個list  分成兩個檔儲存的用意是減少迴圈次數\n",
    "    for key in words_freq:\n",
    "        if words_freq[key] >= minFreq:\n",
    "            brokenwords.update({key:words_freq[key]})\n",
    "    return brokenwords\n",
    "\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "client = pymongo.MongoClient('localhost',27017)\n",
    "db = client.test\n",
    "\n",
    "for dic in db.plaintiff.find():\n",
    "    #print _uniout.unescape(str(dic),'utf-8') \n",
    "    content = dic['plaintiff'] #append all the jurybook to the content\n",
    "    #print len(content)\n",
    "    pk = dic['_id']\n",
    "    #try:\n",
    "    longTermFreq = longTermPriority(content,8,4,1) ##(input_content, maxTermLength, minTermLength, minFrequence)\n",
    "    longTermFreq.update({'_id':pk})\n",
    "    #except:\n",
    "        #f= codecs.open('./ngram/ngramerr.txt','w+' ,'utf-8')\n",
    "        #f.write(pk)\n",
    "        #pass\n",
    "    try:\n",
    "        db2 = client.test2\n",
    "        plaintiff_four = db2['plaintiff_four']\n",
    "        addmongo = db2.plaintiff_four.insert(longTermFreq)\n",
    "    except:\n",
    "        f= codecs.open('./ngram/inserterr.txt','w+' ,'utf-8')\n",
    "        f.write(pk)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge dict and Aggregate values for keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "WriteError",
     "evalue": "new file allocation failure",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mWriteError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-fdfde35fce62>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mmergedic\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mMyDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mmergedic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'_id'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0maggregate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge_320_four\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmergedic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./log/merge_320_four_log.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'a+'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mlog\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miid\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmergedic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\pymongo\\collection.pyc\u001b[0m in \u001b[0;36minsert\u001b[1;34m(self, doc_or_docs, manipulate, check_keys, continue_on_error, **kwargs)\u001b[0m\n\u001b[0;32m   1924\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_socket_for_writes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msock_info\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1925\u001b[0m             return self._insert(sock_info, doc_or_docs, not continue_on_error,\n\u001b[1;32m-> 1926\u001b[1;33m                                 check_keys, manipulate, write_concern)\n\u001b[0m\u001b[0;32m   1927\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1928\u001b[0m     def update(self, spec, document, upsert=False, manipulate=False,\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\pymongo\\collection.pyc\u001b[0m in \u001b[0;36m_insert\u001b[1;34m(self, sock_info, docs, ordered, check_keys, manipulate, write_concern)\u001b[0m\n\u001b[0;32m    429\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".$cmd\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_INSERT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m                 gen(), check_keys, self.codec_options, sock_info)\n\u001b[1;32m--> 431\u001b[1;33m             \u001b[0m_check_write_command_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    432\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m             \u001b[1;31m# Legacy batched OP_INSERT.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\pymongo\\helpers.pyc\u001b[0m in \u001b[0;36m_check_write_command_response\u001b[1;34m(results)\u001b[0m\n\u001b[0;32m    259\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"code\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m11000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mDuplicateKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"errmsg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m11000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 261\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mWriteError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"errmsg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"code\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    262\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m             \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"writeConcernError\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mWriteError\u001b[0m: new file allocation failure"
     ]
    }
   ],
   "source": [
    "class MyDict(dict):\n",
    "    def __add__(self, oth):\n",
    "        #r = self.copy()\n",
    "        r =self\n",
    "        try:\n",
    "            for key, val in oth.items():\n",
    "                if key in r:\n",
    "                    r[key] += val # You can custom it here\n",
    "                else:\n",
    "                    r[key] = val\n",
    "        except AttributeError: # In case oth isn't a dict\n",
    "            return NotImplemented # The convention when a case isn't handled\n",
    "\n",
    "        return r\n",
    "\n",
    "import re,codecs\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import _uniout\n",
    "#make a connection to mongodb \n",
    "client = pymongo.MongoClient('localhost',27017)\n",
    "db = client.test2\n",
    "plaintiff_four = db['plaintiff_four']\n",
    "#make the collection in the db of test\n",
    "merge_320_four = db['merge_320_four']\n",
    "#print the case number in db.dday\n",
    "dicc ={}\n",
    "mergedic =MyDict(dicc)\n",
    "num =1\n",
    "\n",
    "for dic in db.plaintiff_four.find({'_id':{'$regex':'^320_'}},no_cursor_timeout=True):\n",
    "    iid = dic['_id']\n",
    "    del dic['_id']\n",
    "    mergedic += MyDict(dic)\n",
    "    mergedic.update({'_id':num})\n",
    "    aggregate = db.merge_320_four.insert(mergedic)\n",
    "    f = codecs.open('./log/merge_320_four_log.txt','a+','utf-8')\n",
    "    log = iid +'\\t'+ str(len(mergedic)).decode('utf8')+'\\n'\n",
    "    f.write(log)\n",
    "    #print iid,len(mergedic)\n",
    "    num += 1\n",
    "    #print _uniout.unescape(str(mergedic),'utf-8') \n",
    "#print len(mergedic)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mongo input by location and period of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100_88_11_226_890623\n",
      "100_88_11_234_890124\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "client = pymongo.MongoClient('localhost',27017)\n",
    "db = client.test\n",
    "#based on location\n",
    "#for dic in db.samples.find({'_id':{'$regex':'^100_'}}):\n",
    "    #print dic['_id']\n",
    "#based on the year\n",
    "for dic in db.samples.find({'_id':{'$regex':'89(\\d{4})$'}}):\n",
    "    print dic['_id']\n",
    "    #print _uniout.unescape(str(dic),'utf-8') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mongo output the sorted fre to txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#-*- utf-8 -*-\n",
    "import codecs\n",
    "import operator\n",
    "import os\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "client = pymongo.MongoClient('localhost',27017)\n",
    "db = client.test2\n",
    "for ele in db.merge_320_four.find({'_id':2902}):\n",
    "    words_freq = ele\n",
    "result= []\n",
    "text = ''\n",
    "words_freq = sorted(words_freq.iteritems(),key=operator.itemgetter(1),reverse=True)\n",
    "for word in words_freq:\n",
    "    if word[1] >= 2:\n",
    "        result.append(word)\n",
    "for i in result:\n",
    "    text += i[0]+'\\t'+str(i[1]).decode('utf8')+'\\n'\n",
    "f = codecs.open('./log/sinica_four_320.txt','w','utf-8')\n",
    "f.write(text)\n",
    "f.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
